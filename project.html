<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>Project Assignment - Practical Machine Learning JHU - June 2014</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: rgb(88, 72, 246)
   }

   pre .number {
     color: rgb(0, 0, 205);
   }

   pre .comment {
     color: rgb(76, 136, 107);
   }

   pre .keyword {
     color: rgb(0, 0, 255);
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: rgb(3, 106, 7);
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>




</head>

<body>
<h1>Project Assignment - Practical Machine Learning JHU - June 2014</h1>

<p>(By Jaime Abril, 2014)</p>

<pre><code class="r">library(caret)
</code></pre>

<pre><code>## Loading required package: lattice
## Loading required package: ggplot2
</code></pre>

<pre><code class="r">library(randomForest)
</code></pre>

<pre><code>## randomForest 4.6-7
## Type rfNews() to see new features/changes/bug fixes.
</code></pre>

<pre><code class="r">set.seed(1991)
</code></pre>

<h2>The Problem</h2>

<p>We want to predict the manner in which a number of participants have performed a set of workouts using data from several wearable sensors.</p>

<p>The data set comes from the research case:  <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a> (see the section on the Weight Lifting Exercise Dataset).</p>

<p>The data was generated from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.</p>

<h2>The Data</h2>

<p>The training data for this project are available here: </p>

<p><a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a></p>

<p>The testing data are available here: </p>

<p><a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a></p>

<h3>IMPORTANT NOTE:</h3>

<p>In this write-up we will use the following data sets to train and test the predicition models:</p>

<ul>
<li><strong>train</strong> : sample of 70% of the observations contained in the original training data set. This data set will be used to train the models.<br/></li>
<li><strong>test</strong> : sample of remaining 30% of the observations contained in the original training data set. This data set will be used to test the models and estimate the out-of-sample error via cross-validation.<br/></li>
<li><strong>testing</strong> : original testing data set, containing 20 observations, which do not include  the variable to predict (&ldquo;classe&rdquo;) and shall be used to submit the predictions in the Course Project Submission part. </li>
</ul>

<h3>Building the data sets</h3>

<p>We remove the first column of both sets as it is only an enumeration of samples and doesn&#39;t add any information per-se.</p>

<pre><code class="r">training&lt;-read.csv(&quot;http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&quot;)
testing&lt;-read.csv(&quot;http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&quot;)
training&lt;-training[,-1]
testing&lt;-testing[,-1]
</code></pre>

<p>The training data shall be used to train &amp; test the model, hence 
we will split the data into two sets, allocating 70% of the observations to <strong>train data</strong> and the remaining 30% to <strong>test data</strong>: </p>

<pre><code class="r">inTrain&lt;-createDataPartition(training$classe,p=0.7,list=FALSE)
train&lt;-training[inTrain,]
test&lt;-training[-inTrain,]
</code></pre>

<h2>Feature selection</h2>

<p>A quick investigation of the original training &amp; testing data sets using head() and summary() functions reveals the following:  </p>

<ol>
<li>All variables in the training data set which start with max, min, amplitude, avg, var, stddev, kurtosis and skewness have NA values for all observations where the variable <em>new_window</em>==&ldquo;no&rdquo;, and numeric values otherwise (&ldquo;yes&rdquo;).<br/></li>
<li>The variable <em>classe</em> (prediction objective) in the training data set has the same value for all observations where the variable <em>num_window</em> has the same value.<br/></li>
<li>The 20 observations included in the testing data set have all <em>new_window</em>==&ldquo;no&rdquo;, which means all the variables mentioned in point 1. have NA values.<br/></li>
</ol>

<p>Points 1. &amp; 2. above are consistent with the description of the data as described by the authors of the research in: <a href="http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf">http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf</a>, section 5.1:<br/>
&ldquo;For feature extraction we used a sliding window approach with different lengths from 0.5 second to 2.5 seconds, with 0.5 second overlap. In each step of the sliding window approach we calculated features on the Euler angles (roll, pitch
and yaw), as well as the raw accelerometer, gyroscope and magnetometer readings. For the Euler angles of each of the four sensors we calculated eight features: mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness,
generating in total 96 derived feature sets.&rdquo;</p>

<p>From the above investigation we conclude:  </p>

<ol>
<li>Although in the original research the statistical derived features were used as predictors in our particular assignment we cannot use them as their values in all 20 observations in the testing data set are NA values &ndash;&gt; we can ignore these features from the data sets.<br/></li>
<li>Given point 2. above, if the 20 observations in the testing data set are a sample of the collected research data, then the 100% accuracy predictor for variable <em>classe</em> will be the variable <em>num_window</em> &ndash;&gt; we can predict the variable <em>classe</em> with 100% accuracy building a map between <em>num_window</em> and <em>classe</em> out of the testing data.<br/></li>
</ol>

<p>From the conclusion 1. above we can simplify the data sets removing the features we don&#39;t need. We can do this using the grep function as follows:  </p>

<pre><code class="r">trainred&lt;-train[,-c(grep(&quot;^max|^min|^amplitude|^avg|^var|^stddev|^kurtosis|^skewness&quot;,colnames(train)))]
testred&lt;-test[,-c(grep(&quot;^max|^min|^amplitude|^avg|^var|^stddev|^kurtosis|^skewness&quot;,colnames(test)))]
testingred&lt;-testing[,-c(grep(&quot;^max|^min|^amplitude|^avg|^var|^stddev|^kurtosis|^skewness&quot;,colnames(testing)))]
</code></pre>

<p>We are left with 59 variables, including the variable to predict <em>classe</em>. </p>

<p>From the conclusion 2. above, if the underlying assumption is true, we would need only the predictor variable <em>num_window</em>. The algorithm to be used would be simply a mapping <em>num_window</em> -&gt; <em>classe</em>.  </p>

<p>However, if we want an algorithm able to predict the variable <em>classe</em> from brand new input data, the variables <em>num_window</em>, as well as <em>user_name</em>, <em>raw_timestamp_part_1</em>,  <em>raw_timestamp_part_2</em>, <em>cvtd_timestamp</em>, and <em>new_window</em> cannot be used as predictors.</p>

<p>To select which predictor variables to use, out of all available variables, we have several options:  </p>

<h3>Principal Component Analysis (PCA)</h3>

<p>We can calculate the principal components to retain 80% of the data variance as follows:</p>

<pre><code class="r">preProc&lt;-preProcess(trainred[,-c(1,2,3,4,5,6,59)],method=&quot;pca&quot;,thresh=0.8)
trainredPC&lt;-predict(preProc,trainred[,-c(1,2,3,4,5,6,59)])
testredPC&lt;-predict(preProc,testred[,-c(1,2,3,4,5,6,59)])
testingredPC&lt;-predict(preProc,testingred[,-c(1,2,3,4,5,6,59)])
preProc
</code></pre>

<pre><code>## 
## Call:
## preProcess.default(x = trainred[, -c(1, 2, 3, 4, 5, 6, 59)], method
##  = &quot;pca&quot;, thresh = 0.8)
## 
## Created from 13737 samples and 52 variables
## Pre-processing: principal component signal extraction, scaled, centered 
## 
## PCA needed 13 components to capture 80 percent of the variance
</code></pre>

<p>We can use the PCs determined above as predictors. The disadvantage of using PCs as predictors is that a PC is not linked to a particular measure but a combination of measures and the predicting model loses interpretability.</p>

<h3>Search</h3>

<p>Perform a search of which subset of variables would lead to a predicting model which maximises accuracy. This search, if done exhaustively, can be highly time consuming especially when the number of variables is high, like in our case.</p>

<h3>Correlation analysis</h3>

<p>Perform correlation analysis, for example, as suggested in the PhD thesis by Mark A Hall, &ldquo;Correlation-based Feature Selection for Machine Learning&rdquo;, which is the option chosen by the authors of the research paper object of this project. Unfortunately, we don&#39;t have an R implementation of the CFS algorithm ready to be applied to our data, and we cannot use the cor() function as the variable to predict is categorical and the predictors are numeric continuous variables.</p>

<h3>Principal Components variable relevance analysis</h3>

<p>Finally We could make a selection based on how relevant the variables are in the calculation of the PCs obtained from the PCA done before. This can be done by filtering out the rotation matrix elements, obtained from the PCA, which, normalised, are greater than e.g. 0.6, for the first e.g. 5 PCs (which retain the highest variance of the whole PC set):  </p>

<pre><code class="r">which(abs(preProc$rotation[,1:5])/max(abs(preProc$rotation))&gt;0.6,arr.ind=T)
</code></pre>

<pre><code>##                  row col
## roll_belt          1   1
## total_accel_belt   4   1
## accel_belt_y       9   1
## accel_belt_z      10   1
## pitch_belt         2   2
## accel_belt_x       8   2
## magnet_arm_y      25   3
## accel_arm_x       21   4
## magnet_arm_z      26   4
## magnet_forearm_z  52   4
## accel_forearm_y   48   5
</code></pre>

<p>The above variables have a normalised weight greater than 0.6 for the PC1-5 subset.</p>

<h2>Algorithm selection</h2>

<p>From the data analysis and feature selection done above, we will test 3 different algorithms:</p>

<h3>(1) Mapping <em>num_window</em> -&gt; <em>classe</em></h3>

<p>This algorithm should yield 100% accuracy whenever the testing data is a sample of the input data, which seems to be the case as the values of <em>num_window</em> in the testing data are a subset of the values in the training data.<br/>
We can build a mapping predictor variable (x) -&gt; predicted variable (y) as follows:  </p>

<pre><code class="r">xyMapping&lt;-function(x,trainx,trainy,f){
  y&lt;-rep(f,length(x)%/%nlevels(f))
  for (i in 1:length(x))
  {
    y[i]&lt;-trainy[which(trainx==x[i])[1]]
  }
  return(y)
}
</code></pre>

<p>We use the confusionMatrix() function to estimate the out-of-sample error with cross-validation (using the test data set).<br/>
In this case we expect 100% accuracy prediction for the same reason mentioned above.</p>

<pre><code class="r">confusionMatrix(xyMapping(test$num_window,train$num_window,train$classe,factor(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;))),test$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1674    0    0    0    0
##          B    0 1138    0    0    0
##          C    0    0 1024    0    0
##          D    0    0    0  964    0
##          E    0    0    0    0 1082
## 
## Overall Statistics
##                                     
##                Accuracy : 1         
##                  95% CI : (0.999, 1)
##     No Information Rate : 0.285     
##     P-Value [Acc &gt; NIR] : &lt;2e-16    
##                                     
##                   Kappa : 1         
##  Mcnemar&#39;s Test P-Value : NA        
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             1.000    1.000    1.000    1.000    1.000
## Specificity             1.000    1.000    1.000    1.000    1.000
## Pos Pred Value          1.000    1.000    1.000    1.000    1.000
## Neg Pred Value          1.000    1.000    1.000    1.000    1.000
## Prevalence              0.285    0.193    0.174    0.164    0.184
## Detection Rate          0.285    0.193    0.174    0.164    0.184
## Detection Prevalence    0.285    0.193    0.174    0.164    0.184
## Balanced Accuracy       1.000    1.000    1.000    1.000    1.000
</code></pre>

<p>The prediction for the testing data is as follows:</p>

<pre><code class="r">p1&lt;-xyMapping(testing$num_window,train$num_window,train$classe,factor(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;)))
p1
</code></pre>

<pre><code>##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E
</code></pre>

<h3>(2) Random Forest using PCA preprocessed data</h3>

<p>This model is a Random Forest trained using the PCs obtained from the PCA on the train data. As explained before, this model should be able to predict our variable <em>classe</em>  when applied to brand new input data.</p>

<pre><code class="r">g2&lt;-train(trainred$classe ~ ., method=&quot;rf&quot;,data=trainredPC)
</code></pre>

<pre><code>## Loading required package: randomForest
## randomForest 4.6-7
## Type rfNews() to see new features/changes/bug fixes.
</code></pre>

<p>We use the confusionMatrix() function to estimate the out-of-sample error with cross-validation (using the test data set).<br/>
In this case we expect an accuracy below 100%, but relatively high, as we have trained the model with PCs capturaing 80% of the data variance and both train and data sets are large compared to the number of features used.  </p>

<pre><code class="r">confusionMatrix(testred$classe,predict(g2,testredPC))
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1634   18   14    6    2
##          B   22 1084   29    1    3
##          C   12   11  990    9    4
##          D    5    0   35  923    1
##          E    7   14    8    7 1046
## 
## Overall Statistics
##                                        
##                Accuracy : 0.965        
##                  95% CI : (0.96, 0.969)
##     No Information Rate : 0.285        
##     P-Value [Acc &gt; NIR] : &lt; 2e-16      
##                                        
##                   Kappa : 0.955        
##  Mcnemar&#39;s Test P-Value : 1.21e-05     
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.973    0.962    0.920    0.976    0.991
## Specificity             0.990    0.988    0.993    0.992    0.993
## Pos Pred Value          0.976    0.952    0.965    0.957    0.967
## Neg Pred Value          0.989    0.991    0.982    0.995    0.998
## Prevalence              0.285    0.192    0.183    0.161    0.179
## Detection Rate          0.278    0.184    0.168    0.157    0.178
## Detection Prevalence    0.284    0.194    0.174    0.164    0.184
## Balanced Accuracy       0.982    0.975    0.956    0.984    0.992
</code></pre>

<p>The prediction for the testing data is as follows:</p>

<pre><code class="r">p2&lt;-predict(g2,testingredPC)
p2
</code></pre>

<pre><code>##  [1] B A C A A E D B A A B C B A E E A B B B
## Levels: A B C D E
</code></pre>

<p>This prediction differs from the one using the mapping model in the sample(s) number 3</p>

<h3>(3) Random Forest using selected variables</h3>

<p>This model is a Random Forest trained using the selected variables per above PC variable relevance analysis. This model, like the model (2) should be able to predict our variable <em>classe</em>  when applied to brand new input data.</p>

<pre><code class="r">g3&lt;-train(classe ~ roll_belt + total_accel_belt + accel_belt_y + accel_belt_z + pitch_belt + accel_belt_x + magnet_arm_y + accel_arm_x + magnet_arm_z + magnet_forearm_z + accel_forearm_y, method=&quot;rf&quot;,data=trainred)
</code></pre>

<p>We use the confusionMatrix() function to estimate the out-of-sample error with cross-validation (using the test data set).<br/>
In this case we expect an accuracy below 100%, and below the model (2) one above, as we are using a more reduced number of variables than before. </p>

<pre><code class="r">confusionMatrix(testred$classe,predict(g3,testred))
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1607   17   20   17   13
##          B   34 1051   34   12    8
##          C   23   34  946   21    2
##          D   14    8   32  908    2
##          E   12    7    9    4 1050
## 
## Overall Statistics
##                                         
##                Accuracy : 0.945         
##                  95% CI : (0.939, 0.951)
##     No Information Rate : 0.287         
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.931         
##  Mcnemar&#39;s Test P-Value : 0.152         
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.951    0.941    0.909    0.944    0.977
## Specificity             0.984    0.982    0.983    0.989    0.993
## Pos Pred Value          0.960    0.923    0.922    0.942    0.970
## Neg Pred Value          0.980    0.986    0.980    0.989    0.995
## Prevalence              0.287    0.190    0.177    0.163    0.183
## Detection Rate          0.273    0.179    0.161    0.154    0.178
## Detection Prevalence    0.284    0.194    0.174    0.164    0.184
## Balanced Accuracy       0.967    0.961    0.946    0.966    0.985
</code></pre>

<p>The prediction for the testing data is as follows:</p>

<pre><code class="r">p3&lt;-predict(g3,testingred)
p3
</code></pre>

<pre><code>##  [1] B A B A A E D B A A B C B A E B A B B B
## Levels: A B C D E
</code></pre>

<p>This prediction differs from the one using the mapping model in the sample(s) number 16</p>

<h2>Evaluation &amp; Conclusions</h2>

<ol>
<li>The mapping model yields 100% out-of-sample accuracy but wouldn&#39;t be able to generalise for new measurements i.e. new <em>num_window</em> values. However, given that the testing data is a sample of the same input data, we will use this model to make the predictions for the project assignment submission. </li>
<li>The Random Forest model (2), trained with the PCs from the PCA on the train data yields a very high out-of-sample accuracy level of 97%, and is able to generalise. We would use this model with brand new input data. </li>
<li>The Random Forest model (3), trained with the selected variables using PC variable relevance analysis  yields also a very high out-of-sample accuracy level of 95%, and, like model (2) is able to generalise. Given that the variables are direct measures from sensors, this model would allow us to understand better how the variable <em>classe</em> depends on the movement of the user during the workout.<br/></li>
</ol>

</body>

</html>

